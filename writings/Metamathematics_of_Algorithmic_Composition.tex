\documentclass[11pt]{scrartcl}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{scrlayer-scrpage}
\lohead{Michael Gogins}
\rohead{Algorithmic Composition}
\lofoot[Irreducible Productions]
{Irreducible Productions}
\rehead{Michael Gogins}
\lehead{Algorithmic Composition}
\refoot[Irreducible Productions]
{Irreducible Productions}
\usepackage[authordate-trad,backend=biber]{biblatex-chicago}
\addbibresource{gogins.bib}
%\usepackage{cmjStyle} %use CMJ style
%\usepackage{natbib} %natbib package, necessary for customized cmj BibTeX style
%\bibpunct{(}{)}{;}{a}{}{, } %adapt style of references in text
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\begin{document}

\title{Metamathematics of Algorithmic Composition}
\urlstyle{tt}
\author{Michael Gogins \\ \\ \texttt{\href{mailto:michael.gogins@gmail.com}{michael.gogins@gmail.com}}  \\ \url{https://michaelgogins.tumblr.com}}
\date{ May 2025}                                           % Activate to display a given date or no date

\maketitle
\begin{abstract}
This essay recounts my personal journey towards a deeper understanding of the mathematical foundations and computational complexity of algorithmic music composition. I do not spend much time on specific algorithms used by myself or other composers; rather, I focus on general issues such as fundamental limits and possibilities, by analogy with metalogic, metamathematics, and computability theory. I discuss  implications from these foundations for the practice and future of algorithmic composition. 
\end{abstract}

%\section{}
In 1983, when I was a returning undergraduate majoring in comparative religion at the University of Washington, I attended a lecture on fractals by Benoit Mandelbrot, discoverer of the set named after him \parencite{citeulike:580392, peitgen2004mandelbrot}. Briefly, given the quadratic recurrence equation $z_{n+1} = z_n^2 + c$, the Mandelbrot set consists of all points $c$ in the complex plane for which $z$, starting with $z = 0$, does not approach infinity as the equation is iterated. Then, given some particular point $c$ in the Mandelbrot set, there is a Julia set consisting of all points $z$ for which $z_n$ does not approach infinity as the equation is iterated.  Mandelbrot showed slides illustrating how any point in the Mandelbrot set can be used as the generating parameter of a Julia set, and how a plot of the neighborhood near a point in the Mandelbrot set closely resembles the plot of the corresponding Julia set \parencite{lei1990similarity} (this has recently been proved \parencite{kawahira2018julia}). In short, the Mandelbrot set serves as a \emph{parametric map} of all Julia sets. By now there is an extensive literature on the Mandelbrot set and Julia sets, and research continues. 

Already at the time of Mandelbrot's lecture, I was developing an interest in computer music and algorithmic composition, in particular, algorithmic composition based on fractals. What occurred to me during the lecture is that if I zoomed into a plot of the Mandelbrot set, searching for interesting-looking regions, I could then plot the Julia set for a point in that region, and I could then somehow translate that Julia set into a musical score \parencite{obsessed}. I worked out two variations of this idea.

\begin{description}
\item[Mandelbrot Set/Julia Set] The composer explores the actual Mandelbrot set. When a point and its neighborhood seem interesting, the composer selects that point, and the corresponding Julia set is first generated, and then translated to a musical score. The 2-dimensional plot of the Julia set is mapped more or less directly onto a 3-dimensional piano-roll type score, with the $x$ axis representing time, the $y$ axis representing pitch, and the color of a point in the plot representing choice of instrument.

\item[Parametric map of Iterated Function Systems] An iterated function system (IFS) is a Hutchinson operator, a set of contractive affine transformations, and iterating the operator upon any initial set takes that set to a fixed point that is a fractal \parencite{barnsley1985iterated, barnsley1993}. The Collage Theorem proves that an IFS can approximate any set as closely as desired \parencite{barnsley1989fractal, barnsley1993}. It is easy to show that this method is \emph{compositionally universal} \parencite{obsessed, gogins2023scoregraphs}, that is, capable of directly generating any finite score. It is also possible to generate a parametric map of any subset of IFS, but because the Hutchinson operators have many more than two parameters, generating a parametric map (an analogue of the plot of the Mandelbrot set in the complex plane) for dozens or hundreds of parameters requires the use of a Hilbert index \parencite{patrick1968mapping}. A Hilbert index maps an $N$-dimensional space, such as a plane, cube, or hypercube, to a 1- or 2-dimensional sequence of numbers. The index is constructed in such a way that neighboring points in the $N$-dimensional space have nearby indices in the 1- or 2-dimensional sequence. The key idea behind a Hilbert index is to recursively subdivide the $N$-dimensional space into smaller planes, cubes, or hypercubes, called cells. Each cell is assigned a unique index based on its position within the overall space. The subdivision process continues recursively until a desired level of detail is reached. To determine the Hilbert index of a specific point in the $N$-dimensional space, start with the largest cell that contains the point, level $j = 0$. Then, working in arithmetic to the base $N$, subdivide that cell into $N$ sub-cells for level 1, and select the sub-cell that contains the point. If it is the $kth$ sub-cell at level $j$, then add $((k + 1)/N)^{-j}$ to the index. Repeat this process recursively until the smallest cell containing the point is reached. The index of that cell is  the Hilbert index for the point. Hilbert indices work because all metric spaces have the same cardinality; therefore, there is always a one-to-one mapping between points in any $N$-dimensional space and points on the line or, more usefully, the plane. In any event, the plot of the IFS is translated to a score in much the same way as for the plot of a Julia set.
\end{description}

\noindent I experimented with both of these methods, doing parametric composition by zooming in on interesting regions of the map, generating and rendering scores, exploring points near scores that seemed promising, and iterating this process. Unfortunately, the scores I generated from Julia sets had too much of a sameness, and producing a parametric map of more than just a few points for IFS simply took way too much time. In other words, computing parametric maps for IFS quickly became \emph{computationally intractable}. 

As I continued to pursue algorithmic composition, I found that this question of intractability is not merely a practical problem, but has profound mathematical and philosophical roots, beginning with Pythagoras and continuing on through the hierarchy of complexity classes in theoretical computer science.

\section*{The Philosophical Context}

Music has since Pythagoras \parencite{sep-pythagoras, huffman2014history} been understood by some as an intellectual paradigm and to reveal, through harmony that is both numerical and audible, the structure of reality. For this reason music was a central part of the \emph{quadrivium}, the standard curriculum of liberal arts in Western higher education from late Antiquity through the Renaissance.

The project of understanding reality through number advanced from Pythagoras, through Leibniz' 
hope for a \emph{characteristica universalis}, a symbolic language that could express all rational thought \parencite{davis2018universal}, to the logicism of Russell \parencite{sep-logicism}, Hilbert \parencite{sep-hilbert-program}, and others, that sought to derive all mathematical truths from formal logic. In one of the major achievements of all philosophy, Kurt GÃ¶del \parencite{godel1986} demonstrated in his incompleteness theorems that logicism cannot be fully implemented, because there exist true statements of logic that cannot  be proved. About five years later, Alan Turing proved his Halting Theorem: it is impossible for a computer to decide whether another, arbitrary computer program will halt. Working out the consequences of these theorems has taken researchers a long time, and that work continues. 

A major result of this research been the elucidation of a provable hierarchy of complexity classes for problems that are solvable by computer \parencite{arora2009computational}. Other important results are proofs there are completely abstract machines that are not incomplete; these are called super-Turing and depend, one way or another, on doing arithmetic with real numbers that are usually not Turing computable because they never terminate. 

 \subsection*{The Complexity Classes} \label{sec:complexity}
 
 The complexity classes are based on the capabilities and limitations of Turing machines. A Turing machine is an abstract, idealized computer with infinite memory that can run for an infinite number of steps. A universal Turing machine is one that can emulate any other Turing machine. Anything that can be computed by a definite, step by step procedure can be computed by a universal Turing machine; this is the Church-Turing thesis \parencite{sep-church-turing}. Our contemporary digital computers are universal Turing machines -- or would be, if they had infinite memory and infinite time to run.
 
 For our purposes, the important complexity classes, assuming that complexity is defined as run time on a universal Turing machine, are as follows.

\begin{description}
\item[Super-Turing ($\mathsf{ST}$)] Problems that have \emph{mathematical} solutions, but the solutions are not \emph{Turing computable.} 
\item[Recursively enumerable ($\mathsf{RE}$)] Also called semi-decidable. Problems where a solution can be verified by a Turing machine, but proving that a solution does not exist is not always possible. If a solution does exist, then a Turing machine can get as close as one likes to solving the problem.
\item[Recursively computable ($\mathsf{R}$)] This is the same as Turing computable, or decidable. The problem can be decided in a finite number of steps; that is, the problem can be either solved, or proved unsolvable.
\item[Non-deterministic polynomial time ($\mathsf{NP}$)] Recursively computable. The time to solve increases faster than any polynomial function of the size of the problem; however, a solution can be verified in polynomial time.
\item[Deterministic polynomial  time ($\mathsf{P}$)] Computable by a deterministic Turing machine; the time to solve is bounded by some polynomial function of the size of the problem.
\end{description}

The complexity classes, including $\mathsf{ST}$, mirror central issues in philosophy and science. It is a primary open question of philosophy whether Nature herself is super-Turing. If so, then human thought, including musical composition, might as part of Nature also be super-Turing. If not, then human thought is at most $\mathsf{RE}$ and could be emulated as closely as one likes by a Turing machine. 

Yet in either case, scientific theories are $\mathsf{RE}$ or less, because it must be possible to compare the predictions of the theory to observations of Nature that are finite in number and of only finite precision. Nobody can see how Nature or human thought being super-Turing could ever be proved, so it is widely held that any physical process can be effectively emulated by a Turing machine; this is the \emph{physical} Church-Turing thesis \parencite{aaronson2005npcomplete, sep-church-turing}.

The closely related question whether $\mathsf{NP}$ is contained in $\mathsf{P}$ is one of the most important open questions in science. Most mathematicians and scientists believe, for a variety of overlapping reasons, that $\mathsf{P} \ne \mathsf{NP}$ .

If $\mathsf{P} = \mathsf{NP}$  can be proved, it then becomes possible in principle to automatically solve all problems of a size that human beings can grasp. And this means that, given the problem of composing a certain piece of music to specification, a composer is not needed. Algorithmic composition alone can do the job. In terms of parametric composition, computing a finite subset of the parametric map would be in $\mathsf{P}$.

But if $\mathsf{P} \ne \mathsf{NP}$ can be proved, then we will know that there are problems that computers simply cannot solve, but human beings \emph{perhaps} can solve. And this means that, given the problem of composing a certain piece of music to specification, a composer is a good thing to have around. Not only that, but the use of algorithmic composition does not change the benefits of involving a composer.  In terms of parametric composition, that is not because of the indexing, but because of the time required to evaluate a \emph{piece} for each point in the parametric map. Recall that computing a piece as a Julia set may be in $\mathsf{NP}$; therefore, computing a parametric map of pieces also is in $\mathsf{NP}$. However, looking up the set of parameters for a given point in the map is $\mathsf{P}$. It may be only $O(log\ n)$ or, for multi-dimensional searches, $O(n^{c}), 0 < c < 1$.

If neither can be proved, then we will just never know.
 
\subsection*{Algorithmic Composition in Context}

It is now possible to return to the question of parametric composition with more understanding. It turns out that parametric composition spans the entire hierarchy of complexity classes.

\begin{description}
\item[$\mathsf{ST}$]  Braverman proved that some Julia sets, perhaps including some that might be encountered in parametric composition, are not even recursively enumerable \parencite{braverman2006non, braverman2009computability}.
\item[$\mathsf{RE}$] The Mandelbrot set, properly speaking, is not \emph{recursively computable}, i.e.\ not Turing computable \parencite{blum1993godel}. The plots that we make of the set are approximations. Note that incomputability in analysis is related to but not identical with incomputability in logic and theoretical computer science. Hertling showed that although the  Mandelbrot set is not recursively computable, it is nevertheless \emph{recursively enumerable} \parencite{Hertling2005-HERITM-3}; given enough time, one can approximate the actual set as closely as one likes. Most likely all Julia sets that would be of musical interest can be approximated as closely as desired -- are recursively enumerable.
\item[$\mathsf{NP}$] It is also very likely that many of these Julia sets are not computable in polynomial time. If so, computing a parametric map that is big enough to be useful for composers might remain forever computationally intractable.
\item[$\mathsf{P}$] But if $\mathsf{P} = \mathsf{NP}$ then in principle it is possible to produce a useful parametric map in polynomial time. If so, parametric composition might open up a vast new world for composers.
\end{description}

To restate the question, if $\mathsf{P} = \mathsf{NP}$, then a high-resolution parametric map of considerable subsets of compositions would, as it were, display symmetries and patterns, such as those found in the Mandelbrot set, that could assist in parametric composition. It would afford a God's-eye view of possible structure in music, and would partly overcome the irreducibility of algorithmic composition. In other words, $\mathsf{P} = \mathsf{NP}$ could imply that algorithmic composition can be made \emph{intelligible}.

But if $\mathsf{P} \ne \mathsf{NP}$ it would in practice not be possible to produce a high-resolution parametric map of any considerable subset of compositions. And this would fracture and obstruct the intelligibility of algorithmic composition.

Another significant aspect of parametric composition in particular, and of algorithmic composition in general, is \emph{computationally irreducibility}. Although any Julia set approximately resembles the neighborhood of the Mandelbrot set near its generating parameter, almost every Julia set is the chaotic attractor of its generating equation. Therefore, the orbit of the Julia equation is \emph{computationally irreducible} in the sense of Wolfram \parencite{wolfram1985undecidability}, as proved by Zwirn \parencite{zwirn2015computational}. The orbit of the equation cannot be determined by examining the equation, and it cannot except in trivial cases be determined even by mathematically analyzing the equation. In order to know the orbit, it is necessary to actually iterate the equation, that is, to run a program that computes the iterations. Even then, we can only obtain an approximation. What is true of Julia sets is true of IFS or, for that matter, of any algorithm that is not just rudimentary. In order to know what an algorithm actually does, one must actually run the algorithm.

Before further exploring the mathematical foundations of algorithmic composition, I will provide some additional background relating to different software systems for algorithmic composition, and to artificial intelligence, which can also be used to compose music.

\section*{Methods of Algorithmic Composition}

\emph{Algorithmic composition} is the use of computer software to write musical compositions that would be difficult or impossible to write without software. It does not, for example, include the use of notation software to write down music that one hears in one's head (as that can be done with paper and pencil), nor does it include the use of audio workstation software to record and even overdub music that one improvises (as that can be done with a tape recorder). In other words, algorithmic composition consists of all compositionl techniques that are \emph{idiomatic to the computer}. Of course, there is not just one method of algorithmic composition \parencite{fernandez2013ai, arizanet}. A recent summary can be found in \parencite{mclean2018oxford}. However, there is an obvious overlap with a more generic notion of \emph{process music} or \emph{generative music}, including Mozart's musical dice game \parencite{humdrumdice}, the minimalism of Steve Reich \parencite{reichprocess, 10.2307/832600} and Philip Glass \parencite{potter2002four, glass2015words}, and the generative work of Brian Eno \parencite{eno1996generative}. The commonality between algorithmic composition and process music is precisely the simplicity and clarity of the means versus the complexity and unpredictability of the results; in other words, yet again, irreducibility. 

Here I should clarify this idea of irreducibility. It is not a binary choice, it is a spectrum. The minimum of irreducibility occurs when the composer simply writes down what he or she hears in his or her imagination. The maximum occurs when the composition is generated in an entirely random way, so that there is absolutely no way for the composer to predict, better than chance, any particular note or sequence of notes; but even then, there is a degree of musical intelligibility in that the texture of one random variable (e.g.\ white noise) can easily be distinguished from the texture generated by another random variable (e.g.\ brown noise). In the middle of the spectrum is an area where the composer does have some degree of insight into the kind of music that will be generated, even though the details cannot be predicted. And this is the most interesting and most useful degree of computational irreducibility.

Hiller and Isaacson's \emph{Illiac Suite} \parencite{illiacsuite} is the first piece of what can unambiguously be called computer music, and it is an algorithmic composition assembled using a toolkit of stochastic note generators and music transformation functions, as detailed in their book \emph{Experimental Music} \parencite{hiller}. This can be called the \emph{toolkit approach} to algorithmic composition. The composer programs a chain of generators, including random variables, and transformations, including serial transformations, to produce a chunk of music. The chunks can then be edited by hand. Multiple chunks can be assembled into a composition by hand. The toolkit approach lives on in contemporary software systems such as Open Music \parencite{OpenMusic}, Common Music \parencite{CommonMusic, musx}, and many others. This is to date the most successful and widely used method of algorithmic composition.

The more recent method of algorithmic composition known as \emph{live coding} can be considered a variant of the toolkit approach. A live coding system for music consists of a toolkit of routines that are assembled into a music-generating graph during a live performance by interpreting real-time commands in a domain-specific language. Live coding systems have tools and commands for both high-level representations of music (notes, loops, chords, scales, musical transformations, etc.) and sound synthesis (oscillators, envelope generators, filters, etc.). An overview of the field can to some extent be gleaned from the TOPLAP web site \parencite{toplap} and the \emph{Oxford Handbook of Algorithmic Composition} \parencite{mclean2018oxford}. I have some experience with TidalCycles (computer platform, Haskell implementation) \parencite{tidalcycles} and Strudel (a JavaScript version of TidalCycles, Web browser platform, JavaScript implementation) \parencite{strudel}.

Some composers, such as myself, prefer to use an algorithm, such as a Lindenmayer system \parencite{algorithmicbeautyofplants, prusinkiewicz1986sgs,  fractalmusicwithstringrewritinggrammars} or iterated function system (IFS) \parencite{fractalseverywhere, ifsmusic} that will generate an entire piece based on fractals or other mathematical methods, without further editing or assembling. This can be called the \emph{fractal approach} to algorithmic composition.

Recently it has become possible to compose music using generative pre-trained transformers (GPTs) trained with large language models (LLMs). This can be called the \emph{machine learning} approach to computer music. (I prefer the term \emph{machine learning} to \emph{artificial intelligence} because the software is not intelligent, but it is trainable and so in some sense it is learning.) I discuss only LLMs as they are currently the most influential method of machine learning. Briefly, the approach is based on emulating biology, specifically, on simulating at a high level of abstraction the behavior of nerve cells. A neural network is built up from layers of simulated neurons that connect with each other; the connections have tunable weights or parameters that control the output behavior of neurons in one layer given inputs from connected neurons in other layers. The weights are adjusted during training to maximize the value of the result according to some objective fitness function. This value corresponds to the color assigned to the parameter point of a Julia set in a Mandelbrot set. In a GPT, an attention mechanism \parencite{vaswani2017attention} (as each token in the input prompt is processed, a new \emph{context} or summary of model weights is used in place of the entire model) and other heuristic mechanisms have been found to increase the power of the network. In particular, the attention mechanism makes it possible to train the network on a very large body of data without much human intervention. For more detail, see \parencite{zhang2023complete} and OpenAI's paper on their current LLM architecture \parencite{openai2023gpt4}. For working examples of how ChatGPT can be used to compose music, see Jukebox \parencite{openai2023jukebox}, Gonsalves \parencite{aitunes}, and Ocampo et al. \parencite{ocampo2023using}.

Although it is early days for machine learning, contemporary experience has led to a number of reviews and critical studies of the capabilities and limitations of machine learning. From the skeptical side, see \parencite{dale2021gpt}. For an amusing and instructive series of dialogues between all sides, see \parencite{shtetl}. This experience makes it possible to identify a few more important things about the mathematical foundations of algorithmic composition:

\begin{description}
\item[Computational opacity] All agree that ChatGPT can generate amazing, even spooky, results without anyone understanding much about what is going on in the neural network. We have a perfect understanding of each component in the GPT, because these components are actually quite simple, but we have no idea at all how something like ChatGPT can conduct a fact-filled conversation with one in perfect English. The details are scattered through billions of neural network weights in the LLM. Computational opacity is a form of computational irreducibility, but it goes far beyond irreducibility because, with computational opacity, we cannot obtain even a partial understanding of the actual computations performed by the software. We have taken one irreducible program (the GPT) and used it to build another irreducible program (the LLM)! Even though an untrained GPT is computationally irreducible, we still have a perfect understanding of how it actually works; but it seems very likely that we will not, at least in practice, ever obtain even a partial understanding of how the \emph{trained} GPT, i.e.\ the LLM, actually works.
\item [Hallucination] Refers to the tendency of LLMs to generate factually incorrect responses to prompts. It is a reminder that the software has no sense of reality and no means of comparing what it generates with the real world. I suspect that hallucinations arise from the human mistakes, conflicting goals, and outright lies represented in the training data. Ways of dealing with hallucinations are being investigated; for one approach, see \parencite{christiano2017deep}.
\item [Unoriginality] LLMs generate responses to prompts based on high-dimensional correlations that the LLMs have automatically discovered in the training data -- data that \emph{we} have provided. This is a self-referential situation. When we converse with ChatGPT we are looking at ourselves in a mirror; and in fact, in a fun-house mirror.
\end{description}

\section*{Artistic Results and Procedures}

To date, not many pieces of algorithmically composed music have become popular or influential, even among composers and aficionados of art music and experimental music. A few of the pieces that have been influential include Iannis Xenakis' \emph{La LÃ©gende D'Eer} \parencite{Solr-8143160} and \emph{Gendy 3} \parencite{gendy3},  Charles Dodge's \emph{Viola Elegy} \parencite{violaelegy}, and some of Brian Eno's works \parencite{eno1996generative, enochilvers}. I have my own idiosyncratic list of different algorithmic composition systems, with my own choice of representative pieces \parencite{rant}.

The actual procedures followed by composers for algorithmic composition vary by genre, by composer, and by the software used. It is difficult to get a handle on the actual practices of any composers, let alone algorithmic composers. Composing can be a communal effort, as in much contemporary popular music, but art music is usually rather private, and algorithmically composed music even more so. However, IRCAM has published a series of books with chapters by composers on how they have used OpenMusic \parencite{omcomposersbook, agon2006om, agon2008om, agon2016om}. These are very useful. Profiles of composers in \emph{Computer Music Journal} also can be useful. Here I will explain the general procedure that I myself follow.

I start with some kind of mathematical system that can be used to generate a set of musical notes, a score. The system needs to generate complex structure that can be changed by varying a relatively small number of numerical parameters or commands. Itâs often useful to select a recursive algorithm that, as the number of iterations approaches infinity, approaches a fixed point that is a fractal.

Such generative algorithms generally reflect processes in Nature that produce fractal-like forms, such as the patterns on seashells or the branching of plants. I have used chaotic dynamical systems, Lindenmayer systems, iterated function systems, and other systems.

Generally speaking, how to set the parameters in order to obtain a desired result is more or less opaque. This is well-known as the \emph{inverse problem} \parencite{graham2021applying, tu2023learning}. But actually this is another form, once again, of computational irreducibility, meaning in this case that it is not intuitive how to infer the structure of an algorithm even after closely inspecting its results. On the one hand this is a fault of the method; but on the other hand, and even more so, it is a virtue. In this way, and only in this way, can we generate scores that we would not otherwise be able to imagine. This, of course, is another fundamental motivation for pursuing algorithmic composition. And it's the most important motivation. \emph{This kind of algorithmic composition actually amplifies our musical imagination}.

Now the question arises, how can such opaque methods be used to compose good music? It is difficult but by no means impossible, and here is the usual way I do it.

The parameters generally have a global effect on the final structure, that is, on the generated score. For example, an iterated function system consists of a number of affine transformations that are repeatedly applied to a set. Changing just one element of one transformation matrix can easily change every note in the generated score.

So, I pick one parameter and change its value, then listen to the result. I change the value again, and listen to the second result. Then, I choose the value that I prefer. I make more changes and listen again. Eventually I will find a value for that parameter that is more or less optimal â a sweet spot in the parameter space.

Then I pick another parameter and change its value in the same way, until I have a second sweet spot. During this process, the effect of the first parameter will probably change, so I go back to the first parameter and search again for the âsweet spot.â

This can be repeated for any number of parameters, but it is a tedious process and does not make sense for more than a few parameters.

This procedure amounts to a sort of binary search through a set of possible parameter values so vast â indeed infinite â that a linear search is simply out of the question. But a binary search is far more efficient than a linear search. Furthermore, finding two or three ``sweet spots'' in a small set of controlling parameters â each of which has global effects on the entire score â can produce a surprisingly large improvement in the musicality of the result.

I see here an analogy with the way in which LLMs work. There are repeated searches in a parameter space equipped with a fitness function (as with the attention mechanism) at increasing levels of refinement (as with gradient descent).

I have known a number of composers, some quite well. Few algorithmic composers simply ``hear music in their heads'' and write software to render it, although that certainly happens. Most fool around producing various experimental chunks of music, refine them more or less as I have described, and assemble some of them into a finished composition.

Before I proceed to look at this kind of production from a mathematical point of view, I will summarize what I have learned about the mathematical foundations:

\begin{description}
\item[Incomputability] The set of possible musical compositions (assuming that some pieces either last an infinitely long time, or that between any two pieces is a continuous path consisting of variations between the pieces) is \emph{recursively incomputable}.
\item[Universality] In spite of the incomputability of compositions, they are \emph{recursively enumerable}, so it is possible to approximate any possible composition as closely as one likes.
\item[Irreducibility] Compositional algorithms that have a strong analogy to Julia sets are \emph{computationally irreducible}.
\item[Opacity] Compositional algorithms based on machine learning are not only computationally irreducible, but also computationally opaque in that we have essentially no insight into the meaning of the steps followed by the LLM.
\item[Mappability] Compositional algorithms are mappable. This ultimately is because the compositions can be ordered in some way, either as sets of scores, or as sets of generating parameters.
\item[Intractability] Producing a useful parametric map of some subset of compositions is very compute-intensive. 
\item[Hallucination] LLMs that are supposed to provide true or useful outputs sometimes just make stuff up. But this means that material generated by an LLM in response to a prompt cannot be trusted to be true or useful. It is necessary for a person, indeed an expert, to evaluate the material. It is by no means clear at this time whether an expert equipped with a LLM is more productive for creative work than that same expert without the LLM.
\item[Unoriginality] LLMs work by discovering high-dimensional correlations in large bodies of training data. This means that LLMs can select, summarize, and vary --- but they cannot generate an output that is not correlated with the training data. In other words, there is a limit to their originality. However, it is by no means clear at this time whether that limit is well below, or well above, the creativity of experts in the field from which the training data was drawn.
\end{description}

I will now put forward some conjectures based on these foundations.

\subsection*{Limitations}

At this time and for the foreseeable future, no form of artificial intelligence is conscious or has its own goals. Therefore, for the foreseeable future, human composers must and will play a irreplaceable role in algorithmic composition. This involves selecting a subset of possible compositions to study, evaluating the musical quality of each composition in the subset, and varying the parameters or prompts that generate the pieces. This follows from hallucination and unoriginality.

Incomputability, irreducibility, and opacity set objective limits on how much understanding composers can gain into the working of their algorithms and of the music generated by them. This is both a limitation and an advantage. In practice, it is not possible to determine in advance just where those limits lie.

Sophisticated forms of algorithmic composition are compute-intensive, and can be computationally intractable.

\subsection*{Prospects}

Computer power will continue to increase. This will most likely make algorithmic composition both more productive and more important.

There is a similarity between a composer's experience with a toolkit of algorithms, the transformation of prompts into responses by an LLM, and exploring the parametric map of a fractal compositional algorithm. In all cases, starting with an initial trial, a final composition is approached by a descending, zigzag search through a space representing musical possibilities of differing value, until the search comes to rest in some local optimum.

Every method that establishes a more musically compact and/or intelligible ordering of a space of musical possibilities will make algorithmic composition more productive. As an example, excluding non-musical elements from the space of possibilities can be very helpful, because sounds that human beings consider to be musical occupy a vanishingly small part of the parameter space of universal compositional algorithms. However, it is possible to literally change the mathematical basis of the parameter space to represent only musically pertinent features. For example, rather than representing scores as notes on piano rolls, e.g.\ planes or cubes, one can represent scores as more or less fleeting chord progressions in chord spaces \parencite{gogins2006score, gogins2023scoregraphs}.

Every method that speeds up the search process will make algorithmic composition more productive. In particular, the growth of live coding demonstrates that the toolkit approach to algorithmic composition has a future. The underlying reason is that live coding supports faster searching, due to concise commands and immediately audible feedback. Spending time doing live coding also increases the composer's insight into the tools.

\subsection*{In Sum}

The main result here is that the major approaches to algorithmic composition --- trial and error with a toolkit of algorithms, live coding, exploration of fractals, and machine learning --- share this fundamental business of zigzagging down a slope on a landscape of evaluations to rest in a local optimum. This result is proved  by the simple fact that the generated music and/or the parameters used to generate it can be ordered. The dimensionality of the musical space is secondary, as it can be reduced to one or two dimensions by means of a Hilbert index. Note that searching for solutions or optimizations in many domains is known to be computationally expensive.

Future developments in artificial intelligence may have a significant impact on algorithmic composition. For example, machine learning has been applied to solving the inverse problem for discovering the parameters of fractal algorithms \parencite{tu2023learning}. It might then be possible to represent an existing score, or scores, as fractal parameters and then work with these parameters to vary or interpolate between such pieces. This does not overcome computational irreducibility, as it substitutes the opacity of machine learning for the irreducibility of the inverse problem, yet it still might be very useful.

 Algorithmic compositions based on current LLMs are easier to produce, but will not usually be musically original; while algorithmic compositions based on toolkits, live coding, or fractals can be musically original, but are inherently more difficult to produce.

As for algorithmic compositions based on fractals, sometimes an analytical understanding of the mathematics can be used as a guide to composition, but this tends in my experience to be of limited use. More often, the only way to penetrate the fog of incomputability, irreducibility, opacity, and intractability is to explore the geometrical order in a subset of compositions. This can be done either by trial and error, or by literally plotting a map of the subset of compositions. One might say that with trial and error one plots a sparse map of fully defined features in a territory, and with a parametric map one explores a densely mapped territory with partially defined features.

Progress in algorithmic composition seems likely to depend on speeding up the composer's workflow, whether in parametric composition, in live coding, in algorithmic composition toolkits, or in machine learning; and, even more so in the long run, on defining more musically compact and intelligible spaces of musical possibility.

To state this more clearly, the parameters, prompts, or other inputs to the compositional algorithms can be refined such that they are musically intelligible. The processes are not just mathematical, they have a musical meaning. This in my view is the task of future algorithmic composers: identify and refine such processes. 

There are several features of the Mandelbrot set/Julia set duality that are important for a deeper understanding of algorithmic composition not just for these fractals, but for all methods of algorithmic composition. 

%It is also the case that some art music composers and film, game, or popular music composers have incorporated an algorithmic toolkit into their already massive toolkits of music technology.

% Check out https://arxiv.org/abs/1909.11066, the Mandelbrot set is the shadow of a Julia set. Possibly implies that my mapping problems can be solved by projection (shadows).
%\bibliographystyle{cmj}

Parametric map of rule 110

The fixed point of consciousness is not finite

Discrete dynamical systems can perform universal computation (e.g. the rule 110 one-dimensional cellular automaton, or the two-dimensional Game of Life).

Continuous dynamical systems can emulate discrete dynamical systems, so also can perform universal computation. But can a continuous dynamical system perform super-Turing computation? 

\printbibliography
\end{document}  