\documentclass[11pt]{amsart}
%\documentclass[letterpaper, 12pt]{article}

%\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{url}
\usepackage{hyperref}

%\usepackage{cmjStyle} %use CMJ style
%\usepackage{natbib} %natbib package, necessary for customized cmj BibTeX style
%\bibpunct{(}{)}{;}{a}{}{, } %adapt style of references in text

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Mathematical Foundations of Algorithmic Composition}
\author{Michael Gogins}
\date{6 May 2023}                                           % Activate to display a given date or no date

\usepackage[
    backend=biber,
    style=numeric,
    sortlocale=de_DE,
    natbib=true,
    url=false, 
    doi=true,
    eprint=false,
%backend=biber,
bibencoding=utf8,
%style=numeric,
sorting=ynt,
hyperref=true,
backref=true
]{biblatex}
\addbibresource{gogins.bib}

\begin{document}
\maketitle
%\section{}
This essay recounts my personal journey towards a deeper understanding of the mathematical foundations of algorithmic music composition. I also discuss  implications from these foundations for the limitations and prospects of algorithmic composition.

In 1983, when I was a returning undergraduate majoring in comparative religion at the University of Washington, I attended a lecture on fractals by Benoit Mandelbrot, discoverer of the set named after him \cite{citeulike:580392, peitgen2004mandelbrot}. Briefly, given the quadratic recurrence equation $z_{n+1} = z_n^2 + c$, the Mandelbrot set consists of all points $c$ in the complex plane for which $z$, starting with $z = 0$, does not approach infinity as the equation is iterated. Then, given some particular point $c$ in the Mandelbrot set, there is a Julia set consisting of all points $z$ for which $z_n$ does not approach infinity as the equation is iterated.  Mandelbrot showed in his lecture how any point in the Mandelbrot set can be used as the generating parameter of a Julia set, and how a plot of the neighborhood near a point in the Mandelbrot set closely resembles the plot of the corresponding Julia set \cite{lei1990similarity} (this has recently been proved \cite{kawahira2018julia}). In short, the Mandelbrot set serves as a parametric map of all Julia sets. By now there is an extensive literature on the Mandelbrot set and Julia sets, and research continues. 

There are several features of the Mandelbrot set/Julia set duality that are important for a deeper understanding of algorithmic composition.

\begin{description}
\item[Uncomputability] The Mandelbrot set, properly speaking, \emph{is not computable} \cite{blum1993godel}. The plots that we make of the set are approximations. Note that uncomputability in analysis is related to but not identical with uncomputability in logic and theoretical computer science; Hertling showed that although the  Mandelbrot set is not recursively computable, i.e. not Turing computable, the set is nevertheless \emph{recursively enumerable} \cite{Hertling2005-HERITM-3}; that is, given enough time, one can approximate the real set as closely as one likes. There also exist Julia sets that are not computable \cite{braverman2006non, braverman2009computability}.
\item[Computational irreducibility] Although any Julia set approximately resembles the neighborhood of the Mandelbrot set near its generating parameter, any Julia set is the chaotic attractor of its generating equation. Therefore, the orbit of the Julia equation is \emph{computationally irreducible} in the sense of Wolfram \cite{wolfram1985undecidability}, as proved by Zwirn \cite{zwirn2015computational}. The orbit of the equation cannot be determined by examining the equation, and it cannot except in trivial cases be determined even by mathematically analyzing the equation. In order to know the orbit, it is necessary to actually iterate the equation, that is, to run a program that computes the iterations. Even then, we can only obtain an approximation.
\end{description}

Already at the time of Mandelbrot's lecture, I was developing an interest in computer music and algorithmic composition, in particular, algorithmic composition based on fractals. What occurred to me during the lecture is that if one zooms into the plot of the Mandelbrot set, searching for interesting-looking regions, one can then plot the Julia set for a point in that region, and one can then somehow translate that Julia set into a musical score. In general, the 2-dimensional plot is mapped more or less directly onto a 2-dimensional piano-roll type score, with the $x$ axis representing time and the $y$ axis representing pitch.

By iterating this process, one may approach more and more closely to some sort of musically interesting pattern. This is a form of what I have termed parametric composition. Since then I have implemented several variations of this idea in software for composing:

\begin{description}
\item[Map orbits in Julia sets to musical sequences] This was trivial to implement, but the generated music is also trivial.
\item[Map plots of Julia sets to musical scores] This also was trivial to implement, and the generated music is much less trivial, but there are problems with how the plot of the Julia set can best be mapped to a musical score, due to a \emph{dimensional mismatch}. I discuss this further below.
\item[Find a Mandelbrot set for iterated function systems (IFS)] I have proved that this method is \emph{universal} \cite{obsessed, gogins2023scoregraphs}, that is, capable of directly generating any finite score, but the method depends on specifying more than just two parameters, and generating a parametric map (an analogue of the plot of the Mandelbrot set in the complex plane) for dozens or hundreds of parameters becomes \emph{computationally intractable}. I discuss this further below.
\end{description}

I have now added to my list of mathematical things with fundamental implications for algorithmic composition:

\begin{description}
\item[Universality] It is indeed possible to write a computer program that not only can generate any possible score as precisely as one likes, but also to greatly compress the amount of information required to represent the score. This is, of course, one of the fundamental motivations for pursuing algorithmic composition.
\item[Dimensional mismatch] The plot of a Julia set is a fractal in the complex plane. Its fractal dimension is 2 and the complex plane also has dimension 2. Mapping either an orbit in the set, or a plot of the set, to a musical score is frustrating. It can be done heuristically, e.g. by filtering the plot, but it would be elegant if either a musical score had dimension 2, or a Julia set had higher dimensionality, i.e. to directly represent not just pitch and time but also, e.g., loudness and choice of instrument.
\item[Computational intractability] To plot a parametric map of, e.g., IFS that directly represent musical scores is theoretically simple but, in practice, takes much too long to compute given the number of explorations a composer must try in order to find a good piece.
\end{description}

Before further exploring the mathematical foundations of algorithmic composition, I will provide some additional background relating to different software systems for algorithmic composition, and to artificial intelligence, which can also be used to compose music.

\section{Methods of Algorithmic Composition}

First, I will clarify what I mean exactly by \emph{algorithmic composition}. It is the use of computer software to write musical compositions that would be difficult or impossible to write without software. This does not, for example, include the use of notation software to write down music that one hears in one's head (as that can be done with paper and pencil), nor does it include the use of audio workstation software to record and even overdub music that one improvises (as that can be done with a tape recorder). However, there is an obvious overlap with a more generic notion of \emph{process music} or \emph{generative music}, including Mozart's musical dice game \cite{humdrumdice}, the minimalism of Steve Reich \cite{reichprocess, 10.2307/832600} and Philip Glass \cite{potter2002four, glass2015words}, and the generative work of Brian Eno \cite{eno1996generative}. The common thing between algorithmic composition and process music is precisely the simplicity and clarity of the means versus the complexity and unpredictability of the results; in other words, yet again, \emph{irreducibility}. 

Here, I define algorithmic composition as all techniques of composition that are \emph{idiomatic for the computer}. Of course, there is not just one method of algorithmic composition. A recent summary can be found in \cite{mclean2018oxford}.

Hiller and Isaacson's \emph{Illiac Suite} \cite{illiacsuite} is the first piece of what can unambiguously be called computer music, and it is an algorithmic composition assembled using a toolkit of stochastic note generators and music transformation functions, as detailed in their book \emph{Experimental Music} \cite{hiller}. This can be called the \emph{toolkit approach} to algorithmic composition. The composer programs a chain of generators, including random variables, and transformations, including serial transformations, to produce a chunk of music. A chunk can then be edited by hand. Multiple chunks can be assembled into a composition by hand. The toolkit approach lives on in contemporary software systems such as Open Music \cite{OpenMusic}, Common Music \cite{musx}, and so on. This is to date the most successful and widely used method of algorithmic composition.

Some composers, such as myself, prefer to use an algorithm, such as a Lindenmayer system \cite{algorithmicbeautyofplants, prusinkiewicz1986sgs,  fractalmusicwithstringrewritinggrammars} or iterated function system (IFS) \cite{fractalseverywhere, ifsmusic} that will generate an entire piece based on fractals or other mathematical methods, without further editing or assembling. This can be called the \emph{mathematical approach} to algorithmic composition.

Recently it has become possible to compose music using generative pre-trained transformers (GPTs) trained with large language models (LLMs). This can be called the \emph{machine learning} approach to computer music. (I use the term \emph{machine learning} rather than \emph{artificial intelligence} because the software is not intelligent, but it is trainable and so in some sense it is learning.) To explain this technology here, in detail, would be distracting. I will discuss only LLMs as they are currently the dominant method of machine learning. Briefly, the approach is based on emulating biology, specifically, on emulating at a high level of abstraction the behavior of nerve cells. A neural network is built up consisting of layers of neurons that connect with each other; the connections have tunable weights or parameters that control the output behavior of one neuron given inputs from connected neurons. In a GPT, an attention mechanism (as each token in the input prompt is processed, a new \emph{context} or summary of model weights is used in place of the entire model) and other heuristic mechanisms have been found to increase the power of the network. In particular, the attention mechanism makes it possible to train the network on a very large body of data without much human intervention. For more detail, look at \cite{zhang2023complete} and OpenAI's paper on their current LLM architecture \cite{openai2023gpt4}. For working examples of how ChatGPT can be used to compose music, look at Jukebox \cite{openai2023jukebox}, Gonsalves \cite{aitunes}, and Ocampo et al. \cite{ocampo2023using}.

Although it is early days for machine learning, contemporary experience has led to a number of reviews and critical studies of the capabilities and limitations of machine learning. From the skeptical side, see \cite{dale2021gpt}. For an amusing and instructive series of dialogues between all sides, see Scott Aaronson's blog \cite{shtetl}. This experience makes it possible to identify a few more important things about the mathematical foundations of algorithmic composition:

\begin{description}
\item[Computational opacity] All agree that ChatGPT can generate amazing, even spooky, results without anyone understanding much about what is going on in the neural network. We have a perfect understanding of each component in the GPT, because these components are actually quite simple, but we have no idea at all how something like ChatGPT can conduct a fact-filled conversation with one in perfect English. The details are scattered through billions of neural network weights in the LLM. Computational opacity is a form of computational irreducibility, but goes well beyond irreducibility because, with computational opacity, we cannot obtain even a partial understanding of the actual computations performed by the software. We have built one irreducible program (the GPT with pre-trained LLM) on top of another irreducible program (the untrained GPT). Even though it is computationally irreducible, we still have a perfect understanding of how the untrained GPT actually works; but it seems very likely that we will not, at least in practice, ever obtain even a partial understanding of how the  \emph{trained} GPT actually works.
\item [Hallucination] Refers to the tendency of LLMs to produce factually incorrect responses to prompts. It is a reminder that the software has no sense of reality and no means of comparing what it generates with the real world. This may change in the future.
\item [Derivative] LLMs generate responses to prompts based on high-dimensional correlations that the LLMs have automatically discovered in the training data. This is a self-referential process. When we converse with ChatGPT we are looking at ourselves in a mirror, and in fact, in a fun-house mirror.
\end{description}

\section{Artistic Results and Procedures}

To date, few pieces of algorithmically composed music have become popular even among aficionados of art music and experimental music. Some of the pieces that have become popular, or at least influential, include Iannis Xenakis' \emph{La Légende D'Eer} \cite{Solr-8143160}, \emph{Gendy 3} \cite{gendy3}, and Charles Dodge's \emph{Viola Elegy} \cite{violaelegy}, and Brian Eno's \cite{eno1996generative, enochilvers}. I have an idiosyncratic list of different algorithmic composition systems, with my own choice of representative pieces, online \cite{rant}.

The actual procedures followed by composers for algorithmic composition vary by genre, by composer, and by the software used. It is difficult to get a handle on the actual practices of any composers, let alone algorithmic composers. Composing can be a communal effort, as in much contemporary popular music, but art music is usually pretty private, and algorithmically composed music more so. However, IRCAM has published a series of books with chapters by composers on how they have used  \cite{omcomposersbook, agon2006om, agon2008om, agon2016om}. These are very useful. Profiles of composers in \emph\textbf{Computer Music Journal} also can be useful. Here I will explain the general procedure that I myself follow.

I start with some kind of mathematical system that can be used to generate a set of musical notes, a score. The system needs to generate complex structure that can be changed by varying a relatively small number of numerical parameters. It’s often useful to select a recursive algorithm that, as the number of iterations approaches infinity, approaches a fixed point that is a fractal.

Such generative algorithms generally reflect processes in Nature that produce fractal-like forms, such as the patterns on seashells or the branching of plants. I have used chaotic dynamical systems, Lindenmayer systems, iterated function systems, and other systems.

Generally speaking, how to set the parameters in order to obtain a desired result is more or less opaque. This is well-known as the \emph{inverse problem}. Another name for this phenomenon is, once again, \emph{computational irreducibility}, meaning that it is not possible to grasp the results of an algorithm without actually computing it.

This is very far from being a fault of the method, it is a virtue. In this way, and only in this way, we can generate scores that we would not actually be able to imagine. And that’s kind of the whole point of algorithmic composition.

Now the question arises, \emph{how can such opaque methods be used to compose good music}? It is difficult but by no means impossible, and here is the usual way I do it.

The parameters generally have a global effect on the final structure, that is, on the generated score. For example, an iterated function system consists of a number of affine transformations that are repeatedly applied to a set. Changing just one coefficient of one transformation matrix can easily change every note in the generated score.

So, pick one parameter and change its value, then listen to the result. Change the value again, and listen to the second result. Then, choose the value that you prefer. Make more changes and listen again. Eventually you will find a value for that parameter that is more or less optimal – a sweet spot in the parameter space.

Pick another parameter and change its value in the same way, until you have a second sweet spot. During this process, the effect of your first parameter will probably change, so go back to the first parameter and search again for the “sweet spot.”

This can be repeated for any number of parameters, but it is a tedious process and does not make sense for more than a few parameters.

This procedure amounts to a sort of binary search through a set of possible parameter values so vast – indeed infinite – that a linear search is simply out of the question. But a binary search is far more efficient than a linear search. Furthermore, finding two or three “sweet spots” in a small set of controlling parameters – each of which has global effects on the entire score – can produce a surprisingly large improvement in the musicality of the result.

I see here an analogy with the way in which GPTs work. There are repeated searches in a parameter space (as with the attention mechanism) at increasing levels of refinement (gradient descent).

I have known a number of composers, some quite well. Few algorithmic composers simply "hear music in their heads" and write software to render it, although that is certainly done sometimes. Most fool around producing various experimental chunks of music, refine them more or less as I have described, and assemble them into a finished composition.

Before I proceed to look at this kind of production from a mathematical point of view, I will summarize what I have learned about the mathematical foundations:

\begin{description}
\item[Incomputability] 
\item[Universality]
\item[Irreducibility] 
\item[Opacity]
\item[Mapping]
\item[Hallucination]
\item[Unoriginality]
\end{description}



%It is also the case that some art music composers and film, game, or popular music composers have incorporated an algorithmic toolkit into their already massive toolkits of music technology.





% Check out https://arxiv.org/abs/1909.11066, the Mandelbrot set is the shadow of a Julia set. Possibly implies that my mapping problems can be solved by projection (shadows).




%\subsection{}



%\bibliographystyle{cmj}
\printbibliography
\end{document}  