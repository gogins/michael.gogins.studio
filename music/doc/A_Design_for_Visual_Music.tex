\documentclass[english,11pt,letterpaper,onecolumn]{scrartcl}

%\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage{mathptmx}
% Extra leading.
\renewcommand{\baselinestretch}{1.125}
\usepackage{tocloft}
\usepackage{fancyhdr}
\usepackage{scrlayer-scrpage}
\usepackage{ifthen}
\usepackage{keyval}
\usepackage{geometry}
\usepackage{url}
\usepackage{calc}
\usepackage{array}
\usepackage{graphicx}
\usepackage{color}
\usepackage{listings}
\usepackage{supertabular}
\usepackage{scrpage2}
\usepackage[pdftex,
            pagebackref=true,
            colorlinks=true,
            linkcolor=blue,
            pdfpagelabels,
            pdfstartpage=3
           ]{hyperref}
\usepackage{poemscol}
\global\verselinenumbersfalse
\makeindex
\definecolor{LstColor}{cmyk}{0.1,0.1,0,0.025} 
\setcounter{tocdepth}{9}
\begin{document}
\frontmatter

\title{A Framework for Visual Music}
\author{Michael Gogins \\ \texttt{michael.gogins@gmail.com}}
\maketitle
\pagestyle{scrheadings}


\lohead{Visual Music}

\section{Introduction}

Visual music is the production of visual art that is correlated with music 
or sound art. There are two kinds:

\begin{description}
 \item[Real-time] The visuals and the music are both generated in real time, 
for example in a live performance. Both the visuals and the music are dynamic.
 \item[Non-real time] The visuals and the music are generated out of real 
time, as when a composer writes a score that is later performed. The visuals 
are static and the music is dynamic.
\end{description}

Here I am mainly concerned with real-time visual music, but I also consider 
the non-real-time kind, because I have used it heavily in the past.

It is possible to produce the visuals and the music independently and them to 
assemble them, but that is not my concern. I wish to use the same computer 
program to generate both the visuals and the music. It 
would still be possible to generate the visuals and the music independently and 
to use program logic to assemble them, but that is still not my concern. My 
basic approach is to generate the visuals, and then to map and filter them to 
produce some music. This offers many possibilities, yet is fairly simple to 
implement.

I am using mainly csound.node for actual production, but also at times Csound 
for Android or CsoundQt. In all cases the visuals and most of the program 
logic are written as an HTML5 Web page, and Csound is embedded in the page 
along with a Csound orchestra for rendering the music generated by the HTML5 
code.

\section{Bandwidth Disparity}

Both visuals and music are digitally processed at different levels of 
abstraction. 

For visuals, the highest level of abstraction consists of scenes 
of geometric objects or meshes that are covered with textures, illuminated by 
lights, and viewed by a virtual camera; the lowest level of abstraction 
sonsists of a screen of colored pixels, a thousand or so 
wide and less than a thousand high. This size is bound to increase. Up to 60 
or so frames per second are presented. 

A perspective rendering of three 
dimensions is very common, and virtual realities that immerse the viewer in a 
stereoscopic perspective view are becoming more common. But for the purposes 
of visual music, the perspective rendering and the stereoscopic rendering can 
be considered the same, a three-dimensional scene.

For music, the highest level of abstraction consists of scores, which in turn 
consist of notes assigned to instruments, which produce actual streams of 
Mapping audio that are further processed and mixed. There are usually a few to 
a few dozen discrete notes per second. There is an intermediate level of 
abstraction, musical events on a micro scale called grains, that can occur 
at a rate of hundreds or thousands per second. This will require further 
discussion. The lowest level of abstraction is 44,100 to 
96,000 frames of stereo (or, increasingly, multi-channel) audio.

There are obvious disparities between both the types and rates of data between 
the visuals and the music. At the highest level of abstraction, dozens to 
Mapping thousands of visual objects are moving, but only a dozen or so musical 
objects are moving. At the lowest level of abstraction, for uncompressed raw 
data, the bandwidth of high-definition video is on the order of 3,732,480,000 
bits per second, whereas the bandwidth of uncompressed high-definition stereo 
audio is on the order of 4,608,000 bits per second. In reality visual data is 
much more redundant than audio data, so a compressed stream of high-definition 
video runs at about 30,000,000 bits per second, whereas a compressed stream of 
high-definition audio runs at about 500,000 bits per second. So 
compressed visual bandwidth is about 60 times compressed audio 
bandwidth.

The much greater data bandwidth of the visuals is one of 
the reasons it makes sense to derive the music from the visuals, instead of 
the other way round. But therefore it also becomes necessary not only to map 
the visual data to musical parameters, but also to filter or reduce the density 
of data. And this must be done in a way that preserves the ability of the 
audience to relate the visuals with the music.

Finally, the visuals of interest for visual music are not always 
computed as objects in a scene, they may be computed directly at the 
pixel level. This actually is attractive because HTML5 environments can run 
specially compiled ``shader'' programs, which operate directly on pixels, on 
the graphics processing unit (GPU) at much higher speeds than are possible on 
the general purpose central processing unit (CPU). This also will require 
further discussion.

What this all means is that we need to map both fixed and moving images, 
consisting of either objects or pixels, to musical notes or grains. And to 
preserve the intelligibility of the mapping, outstanding features of the 
visuals must be heard as oustanding features of the music.

\section{Mapping}

Mapping visual objects to music is complex and must be considered case by 
case. Here we will discuss only mapping visual pixels to music. The dimensions 
of the pixels are $x, y, h, s, v$ where $x$ and 

\end{document} 
standout