\documentclass[english,11pt,letterpaper,onecolumn]{scrartcl}

%\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage{mathptmx}
% Extra leading.
\renewcommand{\baselinestretch}{1.125}
\usepackage{tocloft}
\usepackage{fancyhdr}
%\usepackage{scrlayer-scrpage}
\usepackage{ifthen}
\usepackage{keyval}
\usepackage{geometry}
\usepackage{url}
\usepackage{calc}
\usepackage{array}
\usepackage{graphicx}
\usepackage{color}
\usepackage{listings}
\usepackage{supertabular}
%\usepackage{scrpage2}
\usepackage[pdftex,
            pagebackref=true,
            colorlinks=true,
            linkcolor=blue,
            pdfpagelabels,
            pdfstartpage=3
           ]{hyperref}
\usepackage{poemscol}
\global\verselinenumbersfalse
\makeindex
\definecolor{LstColor}{cmyk}{0.1,0.1,0,0.025} 
\setcounter{tocdepth}{9}
\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}
\begin{document}

\title{A Framework for Visual Music}
\author{Michael Gogins \\ \texttt{michael.gogins@gmail.com}}
\maketitle
%\pagestyle{scrheadings}


%\lohead{Visual Music}

\section{Introduction}

Visual music is the production of visual art that is correlated with music 
or sound art. There are two kinds:

\begin{description}
 \item[Real-time] The visuals and the music are both generated in real time, 
for example in a live performance. Both the visuals and the music are dynamic.
 \item[Non-real time] The visuals and the music are generated out of real 
time, as when a composer writes a score that is later performed. The visuals 
are static and the music is dynamic.
\end{description}

Here I am mainly concerned with real-time visual music, but I also consider 
the non-real-time kind, because I have used it heavily in the past and plan to 
use it again in the future.

It is possible to produce the visuals and the music independently and them to 
assemble them, but that is not my goal. I wish to use the same computer 
program to generate both the visuals and the music. It 
would still be possible to generate the visuals and the music independently and 
to use program logic to assemble them, but that is still not my goal. My 
basic goal is to generate the visuals, and then to map and filter them to 
produce some music. This offers many possibilities, yet is fairly simple to 
implement.

Let me stress that my primary motivation for this approach is to stay within a 
simple playpen for rapid, iterative composition where I can change the 
program, then instantly both see and hear the results. Experience has taught 
me that although perhaps not as formally powerful as other approaches, this 
approach strengthens the vital connection between programming and artistic 
evaluation, and this is far more important.

I am using mainly csound.node for actual production, but also at times Csound 
for Android or CsoundQt. In all cases the visuals and most of the program 
logic are written as an HTML5 Web page, and Csound is embedded in the page 
along with a Csound orchestra for rendering the music generated by the HTML5 
code.

\section{Bandwidth and Format Disparities}

Both visuals and music are digitally processed at different levels of 
abstraction. 

For visuals, the highest level of abstraction consists of scenes 
of geometric objects or meshes that are covered with textures, illuminated by 
lights, and viewed by a virtual camera; the lowest level of abstraction 
sonsists of a screen of colored pixels, a thousand or so 
wide and less than a thousand high. This size is bound to increase. Up to 60 
or so frames per second are presented. 

A perspective rendering of three 
dimensions is very common, and virtual realities that immerse the viewer in a 
stereoscopic perspective view are becoming more common. But for the purposes 
of visual music, the perspective rendering and the stereoscopic rendering can 
be considered the same: a three-dimensional scene.

For music, the highest level of abstraction consists of scores, which in turn 
consist of notes assigned to instruments, which produce actual streams of 
audio that are further processed and mixed. There are usually a few to 
a some dozens of discrete notes per second. There is an intermediate level of 
abstraction, musical events on a micro scale called grains, that can occur 
at a rate of hundreds or thousands per second. This will require further 
discussion. The lowest level of abstraction is 44,100 to 
96,000 frames per second of stereo (or, increasingly, multi-channel) audio 
samples.

There are obvious disparities between both the formats and rates of data 
between the visuals and the music. At the highest level of abstraction, dozens 
to thousands of visual objects are moving, but no more than a dozen or so 
musical notes are moving. At the lowest level of abstraction, for uncompressed 
raw data, the bandwidth of high-definition video is on the order of 
3,732,480,000 bits per second, whereas the bandwidth of uncompressed 
high-definition stereo audio is on the order of 4,608,000 bits per second. In 
reality visual data is much more redundant than audio data, so a compressed 
stream of high-definition video runs at about 30,000,000 bits per second, 
whereas a compressed stream of high-definition audio runs at about 500,000 bits 
per second. So compressed visual bandwidth is about 60 times compressed audio 
bandwidth.

The much greater data bandwidth of visuals is one reason it makes 
sense to derive the music from the visuals, instead of the other way round. But 
therefore it also becomes necessary not only to map the visual data to musical 
parameters, but also to filter or reduce the density of data -- and this must 
be done in a way that preserves the ability of the audience to relate the 
visuals with the music.

Finally, the visuals of interest for visual music are not always 
computed as objects in a scene; they may be computed directly at the 
pixel level. This actually is attractive, because HTML5 environments can run 
specially compiled ``shader'' programs, which operate directly on pixels, on 
the graphics processing unit (GPU) at \emph{much} higher speeds than are 
possible on the general purpose central processing unit (CPU). This also will 
require further discussion. Indeed, it seems clear that as much processing as 
possible should be done within a fragment shader, which can pass arbitrary 
data back to JavaScript in a pixel buffer.

What this all means is that we need to map both fixed and moving images, 
consisting of either objects or pixels, to musical notes or grains. And to 
preserve the intelligibility of the mapping, outstanding features of the 
visuals must be heard as outstanding features of the music.

\section{Mapping, Filtering, and Triggering}

``Mapping'' actually involves \emph{dimensional mapping}, \emph{filtering} to 
reduce the data bandwidth, and \emph{triggering} musical events. Triggered 
events may in addition be post-processed, e.g.\ to tie overlapping notes, or 
to fit into a harmony.

\subsection{Dimensional Mapping}

Mapping visual \emph{objects} to music is complex, and must be considered case 
by case. Such a mapping amounts to using the visuals as a sort of 
three-dimensional piano roll score for the music. A minimal set of dimensions 
for visual objects might be the following. or even attached invisibly to the 
object. In the following tables, 
we use lower-case letters for visual attributes and upper-case letters for 
musical attributes.

\begin{center}
\begin{tabular}{llll}
$t$ & Time & Real & Seconds from beginning of performance.\\
$x$ & Horizontal Cartesian coordinate & Real & Arbitrary units\\
$y$ & Vertical Cartesian coordinate & Real & Arbitrary units\\
$z$ & Depthwise Cartesian coordinate & Real & Arbitrary units\\
\end{tabular}
\end{center}

\noindent For mapping visual objects to musical events, of course special 
musical attributes can be computed from, or even attached to, the objects in 
the scene, thus reinforcing its double role as a score.

Mapping visual pixels is much more straightforward, as there is the 
following fixed set of dimensions:

\begin{center}
\begin{tabular}{llll}
$t$ & Time & Real & Seconds from beginning of performance\\
$x$ & Horizontal Cartesian coordinate & Integer & 0 to image 
width\\
$y$ & Vertical Cartesian coordinate & Integer & 0 to 
image heght\\
$h$ & Hue & Real & 0 through 1\\
$s$ & Saturation & Real & 0 through 1\\
$v$ & Value (brightness) & Real & 0 through 1\\
\end{tabular}
\end{center}

\noindent The dimensions of notes, at a useful minimum, are:

\begin{center}
\begin{tabular}{llll}
$T$ & Time & Real & Seconds from beginning of performance\\
$I$ & Instrument & Integer & 0 to $I_{Max}$\\
$K$ & MIDI key & Real & $K_{Min}$ through $K_{Max}$\\
$V$ & MIDI velocity & Real & $V_{Min}$ through $V_{Max}$\\
$P$ & Stereo pan & Real & $-1$ through $1$\\
\end{tabular}
\end{center}

\noindent The dimensions of simple grains are somewhat fewer. We assume that 
these are pitch-synchronous overlap-add (PSOLA) cosine grains with cosine 
envelopes, which can overlap with minimal audible artifacts:

\begin{center}
\begin{tabular}{llll}
$T$ & Time & Real & Seconds from beginning of performance\\
$K$ & MIDI key & Real & $K_{Min}$ through $K_{Max}$\\
$V$ & MIDI velocity & Real & $V_{Min}$ through $V_{Max}$\\
$P$ & Stereo pan & Real & $-1$ through $1$\\
\end{tabular}
\end{center}

Given the dimensional units with their minima and maxima, the actual mappings 
are obvious. But we need to distinguish between static visuals and dynamic 
visuals.

For static visuals:

$$T = x / x_{Max} $$
$$I = \floor{1 + h I_{Max} }$$
$$K = \floor{(y / y_{Max}) (K_{Max} - K_{Min}) + K_{Min}} $$
$$V = v (V_{Max} - V_{Min}) + V_{Min}$$
$$P = s 2 - 1 $$

For dynamic visuals:

$$T = t $$
$$I = \floor{1 + x / x_{Max} }$$
$$K = \floor{(y / y_{Max}) (K_{Max} - K_{Min}) + K_{Min}} $$
$$V = v (V_{Max} - V_{Min}) + V_{Min}$$
$$P = s 2 - 1 $$

Implementing these mappings is necessary but not sufficient. The visual 
bandwidth goes straight into a wildly excessive musical bandwidth. The number 
of events must be cut down radically without breaking the audience's 
perception that the visuals are coordinated with the music. 

To accomplish this, let us trigger musical events only from the most salient 
visual events. Then, let us filter the triggers to further cut down the number 
of events.

\subsection{Triggering}

In the retina, a neural network specializes in detecting edges. Here, let us 
similarly use edges to detect easily perceptible events. (In the future, 
perhaps a real computerized neural network could be used to identify features 
at a somewhat higher level of abstraction.)

For static visuals, an edge occurs when, on a single raster, a pixel at 
$x_{i}$ changes color at pixel $x_{i + 1}$. Let us say that when the value 
changes from a level below a threshold, to a level at or above that threshold, 
we trigger a note on event; and when the value changes from a level at or 
above that threshold, to a level below that threshold, we trigger a note off 
event.

For dynamic visuals, an edge occurs when, for a single pixel, the color at 
frame $f_{t}$ changes at time $f_{t+i}$. Let us say that when the value 
changes from a level below a threshold, to a level at or above that threshold, 
we trigger a note on event; and when the value changes from a level at or 
above that threshold, to a level below that threshold, we trigger a note off 
event.

This kind of triggering already filters the visual bandwidth by a considerable 
amount. For static visuals, no new musical events are generated along a raster 
while the color remains stable or the value does not cross the threshold. For 
dynamic visuals, no new musical events are generated at a pixel while its 
color remains stable from frame to frame, or the value does not cross the 
threshold.

However, especially for dynamic visuals, this still creates too many musical 
events per second. Some other kind of filtering must be used in addition to 
the triggering.

\subsection{Filtering}

The simplest triggering/filtering algorithm is to maintain two pixel buffers, 
one containing the data from the prior frame, and one containing data from the 
curent frame. After each frame is rendered, the old pixels are compared with 
the new pixels and, if an edge is detected, an on or off event is triggered 
and added to a set. Actually there are three sets, one for on events, one for 
sounding events, and one for off events. Once the frame is processed, the sets 
are sorted by salience. All but the $N$ most salient on events are discarded 
-- this is the actual filtering -- and the $N$ most salient are dispatched, 
then moved from the on set to the sounding set. The off set is then matched to 
the sounding set, and any matching off events are dispatched; then the 
matching sounding and off events are removed from their sets.

Alternatively, the on events could simply be triggered with durations, which 
could be constant, or mapped from the visuals (saturation?).

Tough sledding here, I have mostly implemented this algorithm in JavaScript 
and so far (a) it is too slow, and (b) there are no custom comparators for Set.

Wait a minute, I am earning how to write efficient JavaScript and I have 
vastly speeded up the color conversion, at least.

Still not sure this is going to work, performance is starting to lag, notes 
are still too thick and I'm not clear on how best to thin them. At least I'm 
learning what the real parameters of the problem are. I suppose I could stride 
through the pixel arrays -- downsample them.



\end{document} 
